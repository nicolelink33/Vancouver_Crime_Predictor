{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VANCOUVER CRIME ANALYSIS PROJECT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        GROUP 21\n",
    "\n",
    "        NICOLE LINK\n",
    "        TIRTH JOSHI\n",
    "        ZAIN NOFAL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this project, we aim to predict what type of crime occurred in Vancouver based on when and where it happened. We use a dataset from the Vancouver Police Department with over 530,000 crime records from 2003-2017, covering 11 different crime types including theft, break-ins, and vehicle collisions.\n",
    "\n",
    "We tested three machine learning models: K-Nearest Neighbors, Support Vector Machines, and Logistic Regression. After tuning, all three models performed similarly, achieving around 62-64% accuracy. While this isn't perfect, it shows that time and location do provide some useful information for predicting crime types, though there is clearly room for improvement, possibly with additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Background\n",
    "\n",
    "Crime prediction is an important tool for police departments trying to figure out where to focus their resources. Vancouver, like most big cities, has many different types of crime happening at different times and places. If we can predict what kind of crime is likely to happen based on patterns in the data, it could help with planning patrols and prevention efforts.\n",
    "\n",
    "### Research Question\n",
    "\n",
    "**Can we predict the type of crime based on when and where it happens?**\n",
    "\n",
    "We're comparing three different classification algorithms (K-NN, SVM, and Logistic Regression) to see if this is possible, and which model works best for this problem.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We're using the Vancouver Crime Dataset from Kaggle, which originally came from the Vancouver Police Department. It has **530,652 crime records** from **2003 to 2017**, split into **11 crime types**. The most common is \"Theft from Vehicle\" (over 172,000 cases) and the rarest is \"Homicide\" (220 cases).\n",
    "\n",
    "Each record includes:\n",
    "- **Time info:** year, month, day, hour, minute\n",
    "- **Location info:** neighborhood, street block, coordinates\n",
    "\n",
    "There's some missing data - about 10% of records don't have time information and 11% are missing neighborhood data. We filled in missing times with the most common values and labeled missing neighborhoods as \"Unknown.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOADING IN THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/zainnofal/.cache/kagglehub/datasets/wosaku/crime-in-vancouver/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandera as pa\n",
    "import json\n",
    "import logging\n",
    "import zipfile\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"wosaku/crime-in-vancouver\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TYPE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>HOUR</th>\n",
       "      <th>MINUTE</th>\n",
       "      <th>HUNDRED_BLOCK</th>\n",
       "      <th>NEIGHBOURHOOD</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other Theft</td>\n",
       "      <td>2003</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9XX TERMINAL AVE</td>\n",
       "      <td>Strathcona</td>\n",
       "      <td>493906.5</td>\n",
       "      <td>5457452.47</td>\n",
       "      <td>49.269802</td>\n",
       "      <td>-123.083763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other Theft</td>\n",
       "      <td>2003</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>9XX TERMINAL AVE</td>\n",
       "      <td>Strathcona</td>\n",
       "      <td>493906.5</td>\n",
       "      <td>5457452.47</td>\n",
       "      <td>49.269802</td>\n",
       "      <td>-123.083763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Other Theft</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>16.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>9XX TERMINAL AVE</td>\n",
       "      <td>Strathcona</td>\n",
       "      <td>493906.5</td>\n",
       "      <td>5457452.47</td>\n",
       "      <td>49.269802</td>\n",
       "      <td>-123.083763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Other Theft</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9XX TERMINAL AVE</td>\n",
       "      <td>Strathcona</td>\n",
       "      <td>493906.5</td>\n",
       "      <td>5457452.47</td>\n",
       "      <td>49.269802</td>\n",
       "      <td>-123.083763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other Theft</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>17.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>9XX TERMINAL AVE</td>\n",
       "      <td>Strathcona</td>\n",
       "      <td>493906.5</td>\n",
       "      <td>5457452.47</td>\n",
       "      <td>49.269802</td>\n",
       "      <td>-123.083763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          TYPE  YEAR  MONTH  DAY  HOUR  MINUTE     HUNDRED_BLOCK  \\\n",
       "0  Other Theft  2003      5   12  16.0    15.0  9XX TERMINAL AVE   \n",
       "1  Other Theft  2003      5    7  15.0    20.0  9XX TERMINAL AVE   \n",
       "2  Other Theft  2003      4   23  16.0    40.0  9XX TERMINAL AVE   \n",
       "3  Other Theft  2003      4   20  11.0    15.0  9XX TERMINAL AVE   \n",
       "4  Other Theft  2003      4   12  17.0    45.0  9XX TERMINAL AVE   \n",
       "\n",
       "  NEIGHBOURHOOD         X           Y   Latitude   Longitude  \n",
       "0    Strathcona  493906.5  5457452.47  49.269802 -123.083763  \n",
       "1    Strathcona  493906.5  5457452.47  49.269802 -123.083763  \n",
       "2    Strathcona  493906.5  5457452.47  49.269802 -123.083763  \n",
       "3    Strathcona  493906.5  5457452.47  49.269802 -123.083763  \n",
       "4    Strathcona  493906.5  5457452.47  49.269802 -123.083763  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data validation: Checking correct data file format\n",
    "# read data in, and throw error if there is an issue with the format\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(os.path.join(path, \"crime.csv\"))\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"File format issue: {e}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVING THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_path = '../data/crimedata.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "zip_path = '../data/crimedata.zip'\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(csv_path, arcname='crimedata.csv')\n",
    "\n",
    "os.remove(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adapted from DSCI 522 lecture notes\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"validation_errors.log\",\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "#Valid crime types\n",
    "crime_types=[\n",
    "    'Other Theft', \n",
    "    'Break and Enter Residential/Other', \n",
    "    'Mischief',\n",
    "    'Break and Enter Commercial', \n",
    "    'Offence Against a Person',\n",
    "    'Theft from Vehicle',\n",
    "    'Vehicle Collision or Pedestrian Struck (with Injury)',\n",
    "    'Vehicle Collision or Pedestrian Struck (with Fatality)',\n",
    "    'Theft of Vehicle', \n",
    "    'Homicide', \n",
    "    'Theft of Bicycle'\n",
    "]\n",
    "\n",
    "# Manually check for and remove privacy protected crime entries:\n",
    "clean_df = df.query(\"HUNDRED_BLOCK != 'OFFSET TO PROTECT PRIVACY'\")\n",
    "\n",
    "# Manually check for and remove duplicate observations\n",
    "dupes = clean_df[clean_df.duplicated()]\n",
    "if not dupes.empty:\n",
    "    logging.warning(f\"{len(dupes)} duplicate row(s) found and removed.\")\n",
    "deduped_df = clean_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Checking for: Correct column names, No empty observations, ...\n",
    "# Correct data types in each column, No outlier or anomalous values, ...\n",
    "# Correct category levels, Missingness not beyond expected threshold of 5%\n",
    "\n",
    "schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"TYPE\": pa.Column(str, pa.Check.isin(crime_types)),\n",
    "        \"YEAR\": pa.Column(int, checks=[\n",
    "            pa.Check.between(2003, 2017),\n",
    "            pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'YEAR' column.\")],\n",
    "                          nullable=True),\n",
    "        \"MONTH\": pa.Column(int, checks=[\n",
    "            pa.Check.between(1, 12),\n",
    "            pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'MONTH' column.\")],\n",
    "                           nullable=True),\n",
    "        \"DAY\": pa.Column(int, checks=[\n",
    "            pa.Check.between(1, 31), \n",
    "            pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'DAY' column.\")],\n",
    "                            nullable=True),\n",
    "        \"HOUR\": pa.Column(float, checks=[\n",
    "            pa.Check.between(0, 24),\n",
    "            pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'DAY' column.\")],\n",
    "                            nullable=True),\n",
    "        \"MINUTE\": pa.Column(float, checks=[\n",
    "            pa.Check.between(0, 60), \n",
    "            pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'MINUTE' column.\")],\n",
    "                            nullable=True),\n",
    "        \"HUNDRED_BLOCK\": pa.Column(str, pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'HUNDRED_BLOCK' column.\"),\n",
    "                            nullable=True),  \n",
    "            # no check on specific levels because current dataset has 21204 unique values\n",
    "        \"NEIGHBOURHOOD\": pa.Column(str, pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'NEIGHBOURHOOD' column.\"),\n",
    "                            nullable=True),\n",
    "            # no check on specific levels because current dataset has 24 unique values, and new neighbourhoods could be added\n",
    "        \"X\": pa.Column(float, checks=[\n",
    "            pa.Check.between(343000, 615910), \n",
    "            pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'X' column.\")],\n",
    "                            nullable=True),\n",
    "            # approximate X & Y UTM coordinates chosen from the following map\n",
    "            # https://coordinates-converter.com/en/decimal/49.120624,-125.156250?karte=OpenStreetMap&zoom=8\n",
    "        \"Y\": pa.Column(float, checks=[\n",
    "            pa.Check.between(5420000, 5530000), \n",
    "            pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'Y' column.\")],\n",
    "                            nullable=True),\n",
    "        \"Latitude\": pa.Column(float, checks=[\n",
    "            pa.Check.between(49, 50),\n",
    "            pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'Latitude' column.\")],\n",
    "                            nullable=True),\n",
    "        \"Longitude\": pa.Column(float, checks=[\n",
    "            pa.Check.between(-125, -121), \n",
    "            pa.Check(lambda s: s.isna().mean() <= 0.05,\n",
    "                                    element_wise=False,\n",
    "                                    error=\"Too many null values in 'Longitude' column.\")],\n",
    "                            nullable=True)\n",
    "    },\n",
    "    checks=[\n",
    "        pa.Check(\n",
    "            lambda df: ~(df.isna().all(axis=1)).any(), \n",
    "            error=\"Empty rows found.\")\n",
    "    ],\n",
    "    drop_invalid_rows=False\n",
    ")\n",
    "\n",
    "# Initialize error cases DataFrame\n",
    "error_cases = pd.DataFrame()\n",
    "\n",
    "# Validate data and handle errors\n",
    "try:\n",
    "    validated_data = schema.validate(deduped_df, lazy=True)\n",
    "except pa.errors.SchemaErrors as e:\n",
    "    error_cases = e.failure_cases\n",
    "\n",
    "    # Convert error message to a JSON string\n",
    "    error_message = json.dumps(e.message, indent=2)\n",
    "    logging.error(\"\\n\" + error_message)\n",
    "\n",
    "# Filter out invalid rows based on the error cases\n",
    "if not error_cases.empty:\n",
    "    invalid_indices = error_cases[\"index\"].dropna().unique()\n",
    "    validated_data = (\n",
    "        data.drop(index=invalid_indices)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    validated_data = deduped_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING & TRANSFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we must process and transform our data, so it is the most useable for modelling. In short, this section creates more detailed features to allow deeper investigation into crime patterns. We split the given datetime into year, month, day, hour, minute columns, and create features for the day of the week, if it's a weekend or not, general time of day, season, if it's late night or not, and create a cyclical encoding for time, found in hour_sin, hour_cos, month_sin, and month_cos. We also calculate the distance of the crime location from downtown Vancouver. \n",
    "\n",
    "Then, to simplify our model's scope, we selected just the top 4 crime types: Theft from Vehicle, Mischief, Break and Enter Residential/Other, and Offence Against a Person. \n",
    "\n",
    "Next, we begin our feature pre-processing by selecting numeric columns to be scaled later, and by performing one hot encoding on our categorical features. \n",
    "\n",
    "Finally, we split our data into training and test sets, using the stratify method to maintain class proportions in both sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                              accuracy_score, f1_score, ConfusionMatrixDisplay)\n",
    "from sklearn.model_selection import (cross_val_score, GridSearchCV, \n",
    "                                RandomizedSearchCV, cross_validate)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Fill missing HOUR and MINUTE values first (using mode)\n",
    "hour_mode = df_processed['HOUR'].mode()[0]\n",
    "minute_mode = df_processed['MINUTE'].mode()[0]\n",
    "\n",
    "df_processed['HOUR'] = df_processed['HOUR'].fillna(hour_mode).astype(int)\n",
    "df_processed['MINUTE'] = df_processed['MINUTE'].fillna(minute_mode).astype(int)\n",
    "\n",
    "# Create a proper datetime column\n",
    "df_processed['DATETIME'] = pd.to_datetime(\n",
    "    df_processed['YEAR'].astype(str) + '-' +\n",
    "    df_processed['MONTH'].astype(str) + '-' +\n",
    "    df_processed['DAY'].astype(str) + ' ' +\n",
    "    df_processed['HOUR'].astype(str) + ':' +\n",
    "    df_processed['MINUTE'].astype(str)\n",
    ")\n",
    "\n",
    "# Extract day of week (0=Monday, 6=Sunday)\n",
    "df_processed['DAY_OF_WEEK'] = df_processed['DATETIME'].dt.dayofweek\n",
    "\n",
    "# Create weekend indicator\n",
    "df_processed['IS_WEEKEND'] = (df_processed['DAY_OF_WEEK'] >= 5).astype(int)\n",
    "\n",
    "# Time of day categories\n",
    "def categorize_time(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    elif 18 <= hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_processed['TIME_OF_DAY'] = df_processed['HOUR'].apply(categorize_time)\n",
    "\n",
    "# Season from month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df_processed['SEASON'] = df_processed['MONTH'].apply(get_season)\n",
    "\n",
    "# Cyclical encoding for time features\n",
    "df_processed['HOUR_SIN'] = np.sin(2 * np.pi * df_processed['HOUR'] / 24)\n",
    "df_processed['HOUR_COS'] = np.cos(2 * np.pi * df_processed['HOUR'] / 24)\n",
    "df_processed['MONTH_SIN'] = np.sin(2 * np.pi * df_processed['MONTH'] / 12)\n",
    "df_processed['MONTH_COS'] = np.cos(2 * np.pi * df_processed['MONTH'] / 12)\n",
    "\n",
    "# Rush hour indicator (morning and evening commute)\n",
    "df_processed['IS_RUSH_HOUR'] = df_processed['HOUR'].apply(\n",
    "    lambda x: 1 if (7 <= x <= 9) or (16 <= x <= 18) else 0\n",
    ")\n",
    "\n",
    "# Late night indicator\n",
    "df_processed['IS_LATE_NIGHT'] = df_processed['HOUR'].apply(\n",
    "    lambda x: 1 if (x >= 22 or x <= 4) else 0\n",
    ")\n",
    "\n",
    "# Distance from downtown Vancouver (approximate coordinates)\n",
    "downtown_x = 491500\n",
    "downtown_y = 5459000\n",
    "df_processed['DIST_FROM_DOWNTOWN'] = np.sqrt(\n",
    "    (df_processed['X'] - downtown_x)**2 +\n",
    "    (df_processed['Y'] - downtown_y)**2\n",
    ")\n",
    "\n",
    "print(\"Feature Engineering Complete\")\n",
    "print(f\"Original columns: {len(df.columns)}\")\n",
    "print(f\"New columns: {len(df_processed.columns)}\")\n",
    "print(f\"Features added: {len(df_processed.columns) - len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of crime types\n",
    "crime_counts = df_processed['TYPE'].value_counts()\n",
    "print(\"Top 10 Crime Types:\")\n",
    "print(crime_counts.head(10))\n",
    "print(f\"\\nTotal crime types in dataset: {len(crime_counts)}\")\n",
    "\n",
    "# Selecting top 4 crime types\n",
    "n_classes = 4\n",
    "selected_crimes = crime_counts.head(n_classes).index.tolist()\n",
    "\n",
    "print(f\"\\nSelected {n_classes} crime types for classification:\")\n",
    "for i, crime in enumerate(selected_crimes, 1):\n",
    "    pct = (crime_counts[crime] / len(df_processed)) * 100\n",
    "    print(f\"{i}. {crime}: {crime_counts[crime]:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Filter dataset\n",
    "df_model = df_processed[df_processed['TYPE'].isin(selected_crimes)].copy()\n",
    "\n",
    "print(f\"\\nFiltered dataset size: {len(df_model):,} records\")\n",
    "print(f\"This represents {len(df_model)/len(df_processed)*100:.1f}% of all crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    'HOUR', 'DAY_OF_WEEK', 'MONTH', 'DAY',\n",
    "    'IS_WEEKEND', 'IS_RUSH_HOUR', 'YEAR', 'IS_LATE_NIGHT',\n",
    "    'HOUR_SIN', 'HOUR_COS', 'MONTH_SIN', 'MONTH_COS',\n",
    "    'DIST_FROM_DOWNTOWN', 'X', 'Y'\n",
    "]\n",
    "\n",
    "categorical_cols = ['NEIGHBOURHOOD', 'TIME_OF_DAY', 'SEASON']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPLITTING THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y = df_model['TYPE']\n",
    "\n",
    "# Split into training and test sets first\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_model[numeric_cols + categorical_cols], y,\n",
    "    test_size=0.2,\n",
    "    random_state=522,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples ({len(X_train)/len(df_model)*100:.0f}%)\")\n",
    "print(f\"Test set: {len(X_test):,} samples ({len(X_test)/len(df_model)*100:.0f}%)\")\n",
    "\n",
    "print(\"\\nClass balance in splits:\")\n",
    "print(\"\\nTraining:\")\n",
    "for crime in selected_crimes:\n",
    "    count = (y_train == crime).sum()\n",
    "    pct = count / len(y_train) * 100\n",
    "    print(f\"  {crime}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nTest:\")\n",
    "for crime in selected_crimes:\n",
    "    count = (y_test == crime).sum()\n",
    "    pct = count / len(y_test) * 100\n",
    "    print(f\"  {crime}: {count:,} ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric features\n",
    "X_train_numeric = X_train[numeric_cols].copy()\n",
    "X_test_numeric = X_test[numeric_cols].copy()\n",
    "\n",
    "\n",
    "# Categorical features\n",
    "X_train_cat = X_train[categorical_cols].copy()\n",
    "X_test_cat = X_test[categorical_cols].copy()\n",
    "X_train_eda = X_train_numeric.join(X_train_cat) \n",
    "\n",
    "# One-hot encode training categorical features\n",
    "X_train_cat_encoded = pd.get_dummies(X_train_cat, drop_first=True)\n",
    "\n",
    "# Align test set to train set columns (handles unseen categories)\n",
    "X_test_cat_encoded = pd.get_dummies(X_test_cat, drop_first=True)\n",
    "X_test_cat_encoded = X_test_cat_encoded.reindex(columns=X_train_cat_encoded.columns, fill_value=0)\n",
    "\n",
    "# Combine numeric + categorical\n",
    "X_train = pd.concat([X_train_numeric, X_train_cat_encoded], axis=1)\n",
    "X_test = pd.concat([X_test_numeric, X_test_cat_encoded], axis=1)\n",
    "\n",
    "# Store numeric column indices for scaling later\n",
    "numeric_feature_indices = list(range(len(numeric_cols)))\n",
    "\n",
    "print(f\"\\nFeature matrix shape (train): {X_train.shape}\")\n",
    "print(f\"Feature matrix shape (test): {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nTarget distribution in training set:\")\n",
    "for crime in selected_crimes:\n",
    "    count = (y_train == crime).sum()\n",
    "    pct = count / len(y_train) * 100\n",
    "    print(f\"  {crime}: {count:,} ({pct:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FINAL DATA VALIDATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Panderas Check: Checking that target/response variable follows expected distribution\n",
    "Since our target variable is categorical, and does not have a theoretical expected distribution it should follow, we check that each of our 4 top crime categories makes up at least 10% of the total training set. This way we ensure that all 4 crimes have a significant number of observations present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = y_train.value_counts(normalize=True)\n",
    "for crime in selected_crimes:\n",
    "    observed_percent = train_counts.get(crime, 0)\n",
    "    \n",
    "    if observed_percent < 0.10:\n",
    "        message = (f”Distribution check failed - observations of {crime} make up less than 10% of the training dataset”)\n",
    "        logging.warning(message)\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**USING DEEPCHECKS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(X_train, label=y_train)\n",
    "test_ds = Dataset(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anomalous Correlations Between Features and Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import FeatureLabelCorrelation\n",
    "\n",
    "check = FeatureLabelCorrelation().add_condition_feature_pps_less_than(0.9)\n",
    "result = check.run(train_ds)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detecting Anamolous Correlations Among Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import FeatureFeatureCorrelation\n",
    "\n",
    "check = FeatureFeatureCorrelation()\n",
    "result = check.run(train_ds)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eda.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eda.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values\n",
    "X_train_eda.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()\n",
    "#alt.data_transformers.enable(\"vegafusion\")\n",
    "# added type to this copy df\n",
    "train_plot_df = X_train_eda.copy()\n",
    "train_plot_df['TYPE'] = y_train.values\n",
    "\n",
    "alt.Chart(train_plot_df).mark_bar().encode(\n",
    "     y=alt.Y('TYPE:N', sort='-x', title='Crime Type'),\n",
    "     x = alt.X('count()', title = 'Number of Crimes')\n",
    ").properties(\n",
    "    title = 'Distribution of Crime Types',\n",
    "    width = 800\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1. Frequency of type of crime\n",
    "\n",
    "We can see that the most frequent crime by far is Theft from Vehicle, followed by Mischief, Break and Enter Residential/Other, and Offence Against a Person, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(X_train_eda).mark_line(point=True).encode(\n",
    "    x=alt.X('YEAR:O', title='Year'),\n",
    "    y=alt.Y('count()', title='Number of Crimes')\n",
    "    ).properties(\n",
    "    title='Crime Trend Over Years',\n",
    "    width=800\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2. Trend in number of crimes over time. \n",
    "\n",
    "We can see that crime rates declined from 2003 to 2011, then increased again until 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(X_train_eda).mark_bar().encode(\n",
    "    x=alt.X('HOUR:O', title='Hour of Day'),\n",
    "    y=alt.Y('count()', title='Number of Crimes'),\n",
    ").properties(\n",
    "    title='Crimes by Hour of Day',\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3. Number of crimes recorded by hour of day. \n",
    "\n",
    "We can see that early morning hours have the lowest crime rates, and 6:00pm has, by far, the highest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chart = alt.Chart(X_train_eda).mark_bar().encode(\n",
    "    y=alt.Y('NEIGHBOURHOOD:N', sort='-x', title='Neighbourhood'),\n",
    "    x=alt.X('count()', title='Number of Crimes'),\n",
    ").properties(\n",
    "    title='Top 10 Neighbourhoods by Crime Count',\n",
    "    width=700,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "chart\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4. Number of crimes recorded by neighborhood. \n",
    "\n",
    "For neighborhoods, we can see that the Central Business District has the most crimes recorded, followed by our created 'Unknown' category, then the West End. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# centering the map around vancouver\n",
    "m = folium.Map(location=[49.2827, -123.1207], zoom_start=12)\n",
    "\n",
    "# adding markers for the first 2000 to avoid it being crowded\n",
    "for idx, row in df.head(2000).iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        radius=3,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_opacity=0.5,\n",
    "        popup=f\"{row['TYPE']} - {row['NEIGHBOURHOOD']}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5. Geographical map of location of all reported crimes in our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KNN ANALYSIS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will perform our model analysis using the k-nearest neighbors model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_numeric = len(numeric_cols)\n",
    "numeric_indices = list(range(num_numeric))\n",
    "onehot_indices = list(range(num_numeric, X_train.shape[1]))\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_indices),\n",
    "        ('cat', 'passthrough', onehot_indices)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Baseline model\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "y_pred = baseline_pipeline.predict(X_test)\n",
    "baseline_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Baseline KNN (k=5): {baseline_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for faster k-value testing\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "sample_size = 15000\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(\n",
    "    X_train, y_train,\n",
    "    train_size=sample_size,\n",
    "    stratify=y_train,\n",
    "    random_state=522\n",
    ")\n",
    "\n",
    "print(f\"Using {len(X_train_sample):,} samples for k-value search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-value optimization\n",
    "\n",
    "k_values = list(range(5, 101, 5))\n",
    "scores = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for k in tqdm(k_values, desc=\"Testing k values\"):\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=k))\n",
    "    ])\n",
    "    cv_score = cross_val_score(pipeline, X_train_sample, y_train_sample, \n",
    "                               cv=3, scoring='accuracy', n_jobs=-1).mean()\n",
    "    scores.append(cv_score)\n",
    "\n",
    "best_k = k_values[np.argmax(scores)]\n",
    "best_score = max(scores)\n",
    "\n",
    "print(f\"\\nBest k: {best_k} (CV accuracy: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_values, scores, 'o-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=best_k, color='red', linestyle='--', label=f'Best k={best_k}')\n",
    "plt.xlabel('k (number of neighbors)')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.title('Finding the optimal k value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 6. Accuracy of k-NN model for different values of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model with optimal k\n",
    "final_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=best_k))\n",
    "])\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "final_acc = accuracy_score(y_test, y_pred_final)\n",
    "\n",
    "print(f\"\\nFinal KNN Model (k={best_k})\")\n",
    "print(f\"Test accuracy: {final_acc:.4f}\")\n",
    "print(f\"Baseline (k=5): {baseline_acc:.4f}\")\n",
    "print(f\"Improvement: {(final_acc - baseline_acc):.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=selected_crimes)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "\n",
    "plt.title(f'Confusion Matrix - KNN (k={best_k})', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Crime Type', fontsize=12)\n",
    "plt.ylabel('Actual Crime Type', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze where the model makes mistakes\n",
    "print(\"\\nModel Performance Analysis:\")\n",
    "print(\"=\"*60)\n",
    "for i, crime in enumerate(selected_crimes):\n",
    "    tp = cm[i, i]\n",
    "    total_actual = cm[i, :].sum()\n",
    "    recall = tp / total_actual if total_actual > 0 else 0\n",
    "    print(f\"\\n{crime}:\")\n",
    "    print(f\"  Correctly predicted: {tp}/{total_actual} ({recall:.1%})\")\n",
    "    \n",
    "    # Find most common misclassification\n",
    "    misclass_idx = [j for j in range(len(selected_crimes)) if j != i and cm[i, j] > 0]\n",
    "    if misclass_idx:\n",
    "        max_misclass_idx = max(misclass_idx, key=lambda j: cm[i, j])\n",
    "        print(f\"  Most often confused with: {selected_crimes[max_misclass_idx]} ({cm[i, max_misclass_idx]} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 7. Confusion matrix for k-NN model with k=85, showing predicted crime type versus actual crime type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs optimized model performance\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [f'Baseline (k=5)', f'Optimized (k={best_k})'],\n",
    "    'Accuracy': [baseline_acc, final_acc],\n",
    "    'Improvement': ['—', f'+{((final_acc - baseline_acc) / baseline_acc * 100):.2f}%']\n",
    "})\n",
    "# Visualize the comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(comparison_df['Model'], comparison_df['Accuracy'], \n",
    "              color=['#87CEEB', '#90EE90'], edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('KNN Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"KNN RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Baseline (k=5): {baseline_acc:.1%}\")\n",
    "print(f\"Optimized (k={best_k}): {final_acc:.1%}\")\n",
    "print(f\"Improvement: +{(final_acc - baseline_acc):.1%}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 8. Comparing accuracies of baseline k-NN model (k=5) and optimized k-NN model (k=85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Analysis Summary\n",
    "\n",
    "**Model Performance:**\n",
    "- Started with k=5 baseline: 58.8% accuracy\n",
    "- Optimized to k=85: 64.2% accuracy  \n",
    "- 5.5% improvement through hyperparameter tuning\n",
    "\n",
    "**Key Findings:**\n",
    "- KNN works well for this crime classification problem\n",
    "- Spatial and temporal features have predictive power\n",
    "- Model performs best on \"Theft from Vehicle\" (93% recall)\n",
    "- \"Mischief\" and \"Break and Enter\" are harder to predict\n",
    "\n",
    "**Technical Notes:**\n",
    "- Used ColumnTransformer to scale only numeric features (no golden rule violations)\n",
    "- Cross-validation on 15k sample for efficient k-value selection\n",
    "- Final model trained on full dataset with optimal k=85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM ANALYSIS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we begin with creating a baseline linear SVM model, using the default value of 1 for the hyperparameter C. We will then perform hyperparameter optimization for C, and compare our results to this baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passthrough_feats = ['NEIGHBOURHOOD_Central Business District',\n",
    " 'NEIGHBOURHOOD_Dunbar-Southlands',\n",
    " 'NEIGHBOURHOOD_Fairview',\n",
    " 'NEIGHBOURHOOD_Grandview-Woodland',\n",
    " 'NEIGHBOURHOOD_Hastings-Sunrise',\n",
    " 'NEIGHBOURHOOD_Kensington-Cedar Cottage',\n",
    " 'NEIGHBOURHOOD_Kerrisdale',\n",
    " 'NEIGHBOURHOOD_Killarney',\n",
    " 'NEIGHBOURHOOD_Kitsilano',\n",
    " 'NEIGHBOURHOOD_Marpole',\n",
    " 'NEIGHBOURHOOD_Mount Pleasant',\n",
    " 'NEIGHBOURHOOD_Musqueam',\n",
    " 'NEIGHBOURHOOD_Oakridge',\n",
    " 'NEIGHBOURHOOD_Renfrew-Collingwood',\n",
    " 'NEIGHBOURHOOD_Riley Park',\n",
    " 'NEIGHBOURHOOD_Shaughnessy',\n",
    " 'NEIGHBOURHOOD_South Cambie',\n",
    " 'NEIGHBOURHOOD_Stanley Park',\n",
    " 'NEIGHBOURHOOD_Strathcona',\n",
    " 'NEIGHBOURHOOD_Sunset',\n",
    " 'NEIGHBOURHOOD_Victoria-Fraserview',\n",
    " 'NEIGHBOURHOOD_West End',\n",
    " 'NEIGHBOURHOOD_West Point Grey',\n",
    " 'TIME_OF_DAY_Evening',\n",
    " 'TIME_OF_DAY_Morning',\n",
    " 'TIME_OF_DAY_Night',\n",
    " 'SEASON_Spring',\n",
    " 'SEASON_Summer',\n",
    " 'SEASON_Winter']\n",
    "\n",
    "svm_transf = make_column_transformer(\n",
    "    (StandardScaler(), numeric_cols),\n",
    "    (\"passthrough\", passthrough_feats)\n",
    ")\n",
    "\n",
    "svm_base_pipe = make_pipeline(\n",
    "    svm_transf,\n",
    "    LinearSVC(C=1)\n",
    ")\n",
    "\n",
    "svm_base_pipe.fit(X_train, y_train)\n",
    "svm_base_pred = svm_base_pipe.predict(X_test)\n",
    "svm_base_acc = accuracy_score(y_test, svm_base_pred)\n",
    "\n",
    "print(f\"Baseline SVM (C=1) accuracy: {svm_base_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter optimization for gamma and C\n",
    "#using the X_train_sample and y_train_sample from k-NN testing to speed up optimization\n",
    "\n",
    "svm_pipe = make_pipeline(\n",
    "    svm_transf,\n",
    "    LinearSVC()\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"linearsvc__C\": [0.001, 0.01, 0.5, 0.1, 1, 10, 50, 100]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    svm_pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "svm_grid.fit(X_train_sample, y_train_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid_results = pd.DataFrame(svm_grid.cv_results_\n",
    "    ).set_index(\"rank_test_score\"\n",
    "    ).sort_index()\n",
    "svm_grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how accuracy changes with C\n",
    "alt.Chart(svm_grid_results, \n",
    "    title=alt.Title(\n",
    "        text=\"Finding Optimal C Value for Linear SVM by Grid Search\"\n",
    "    )\n",
    ").mark_circle(size=70\n",
    ").encode(\n",
    "    x=alt.X('param_linearsvc__C').title(\"C Value\"),\n",
    "    y=alt.Y('mean_test_score').scale(zero=False\n",
    "            ).title(\"Cross-Validation Accuracy\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 9. Accuracies of linear SVM models with wide range of C values. \n",
    "\n",
    "From the above grid search, we can see that the optimal values of C fall below 1, and above that performance does not seem to change. We can now perform a randomized search within that optimal range to find the best C value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"linearsvc__C\": loguniform(1e-3, 1)\n",
    "}\n",
    "\n",
    "svm_random_search = RandomizedSearchCV(svm_pipe, \n",
    "    param_distributions = param_dist, \n",
    "    n_iter=10, \n",
    "    cv=5,\n",
    "    n_jobs=-1, \n",
    "    return_train_score=True)\n",
    "\n",
    "svm_random_search\n",
    "\n",
    "svm_random_search.fit(X_train_sample, y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results = pd.DataFrame(svm_random_search.cv_results_\n",
    "    ).set_index(\"rank_test_score\"\n",
    "    ).sort_index()\n",
    "svm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how accuracy changes with C\n",
    "alt.Chart(svm_results, \n",
    "    title=alt.Title(\n",
    "        text=\"Finding Optimal C Value for Linear SVM\"\n",
    "    )\n",
    ").mark_circle(size=70\n",
    ").encode(\n",
    "    x=alt.X('param_linearsvc__C').title(\"C Value\"),\n",
    "    y=alt.Y('mean_test_score').scale(zero=False\n",
    "            ).title(\"Cross-Validation Accuracy\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 10. Accuracies of linear SVM models with targetted range of C values. \n",
    "\n",
    "We can see that the optimal C values seem to be around 0.01, below which and above which performance suffers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = svm_random_search.best_params_['linearsvc__C']\n",
    "best_score = svm_random_search.best_score_\n",
    "\n",
    "print(f\"Best C value: {best_C:.4f}\")\n",
    "print(f\"Cross-validation accuracy: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM: Testing smaller dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has many columns, some of which contain overlapping data (eg. latitude/longitude and neighborhood). I wanted to investigate if removing some columns would impact model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_numeric = ['HOUR',\n",
    " 'DAY_OF_WEEK',\n",
    " 'MONTH',\n",
    " 'DAY',\n",
    " 'X',\n",
    " 'Y',\n",
    " 'IS_WEEKEND', 'IS_RUSH_HOUR',\n",
    " 'IS_LATE_NIGHT', 'HOUR_SIN', 'HOUR_COS', 'MONTH_SIN', 'MONTH_COS',\n",
    " 'DIST_FROM_DOWNTOWN']\n",
    "\n",
    "drop_feats = [\n",
    " 'NEIGHBOURHOOD_Central Business District',\n",
    " 'NEIGHBOURHOOD_Dunbar-Southlands',\n",
    " 'NEIGHBOURHOOD_Fairview',\n",
    " 'NEIGHBOURHOOD_Grandview-Woodland',\n",
    " 'NEIGHBOURHOOD_Hastings-Sunrise',\n",
    " 'NEIGHBOURHOOD_Kensington-Cedar Cottage',\n",
    " 'NEIGHBOURHOOD_Kerrisdale',\n",
    " 'NEIGHBOURHOOD_Killarney',\n",
    " 'NEIGHBOURHOOD_Kitsilano',\n",
    " 'NEIGHBOURHOOD_Marpole',\n",
    " 'NEIGHBOURHOOD_Mount Pleasant',\n",
    " 'NEIGHBOURHOOD_Musqueam',\n",
    " 'NEIGHBOURHOOD_Oakridge',\n",
    " 'NEIGHBOURHOOD_Renfrew-Collingwood',\n",
    " 'NEIGHBOURHOOD_Riley Park',\n",
    " 'NEIGHBOURHOOD_Shaughnessy',\n",
    " 'NEIGHBOURHOOD_South Cambie',\n",
    " 'NEIGHBOURHOOD_Stanley Park',\n",
    " 'NEIGHBOURHOOD_Strathcona',\n",
    " 'NEIGHBOURHOOD_Sunset',\n",
    " 'NEIGHBOURHOOD_Victoria-Fraserview',\n",
    " 'NEIGHBOURHOOD_West End',\n",
    " 'NEIGHBOURHOOD_West Point Grey',\n",
    " 'TIME_OF_DAY_Evening',\n",
    " 'TIME_OF_DAY_Morning',\n",
    " 'TIME_OF_DAY_Night',\n",
    " 'SEASON_Spring',\n",
    " 'SEASON_Summer',\n",
    " 'SEASON_Winter'\n",
    " ]\n",
    "\n",
    "small_svm_transf = make_column_transformer(\n",
    "    (StandardScaler(), small_numeric),\n",
    "    (\"drop\", drop_feats)\n",
    ")\n",
    "\n",
    "small_svm_pipe = make_pipeline(\n",
    "    small_svm_transf,\n",
    "    LinearSVC()\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    \"linearsvc__C\": loguniform(1e-3, 1e2)\n",
    "}\n",
    "\n",
    "\n",
    "small_svm_random_search = RandomizedSearchCV(small_svm_pipe, \n",
    "    param_distributions = param_grid, \n",
    "    n_iter=10, \n",
    "    cv=5,\n",
    "    n_jobs=-1, \n",
    "    return_train_score=True)\n",
    "\n",
    "small_svm_random_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_svm_random_search.fit(X_train_sample, y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_svm_results = pd.DataFrame(small_svm_random_search.cv_results_\n",
    "    ).set_index(\"rank_test_score\"\n",
    "    ).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how accuracy changes with C\n",
    "alt.Chart(small_svm_results, \n",
    "    title=alt.Title(\n",
    "        text=\"Finding Optimal C Value for Linear SVM\"\n",
    "    )\n",
    ").mark_circle(size=70\n",
    ").encode(\n",
    "    x=alt.X('param_linearsvc__C').title(\"C Value\"),\n",
    "    y=alt.Y('mean_test_score').scale(zero=False\n",
    "            ).title(\"Cross-Validation Accuracy\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 11. Accuracies of linear SVM models using a simplified dataset, across a wide range of C values. \n",
    "\n",
    "We're not seeing much change in accuracy or performance of our model whether we drop some of the excess columns. Since no difference is observed, we will keep these columns in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final SVM Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training final model with best C\n",
    "\n",
    "final_svm = make_pipeline(\n",
    "    svm_transf,\n",
    "    LinearSVC(C=best_C)\n",
    ")\n",
    "final_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring final model\n",
    "final_svm_pred = final_svm.predict(X_test)\n",
    "\n",
    "svm_score = final_svm.score(X_test, y_test)\n",
    "print(f\"Final SVM Model (C={best_C:.4f})\")\n",
    "print(f\"Test Accuracy: {svm_score:.4f}\")\n",
    "\n",
    "print(\"\\nDetailed Classification Report\")\n",
    "print(classification_report(y_test, final_svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from k-NN analysis visualization - created by Tirth\n",
    "\n",
    "# Confusion matrix\n",
    "cm_svm = confusion_matrix(y_test, final_svm_pred)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=selected_crimes)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "\n",
    "plt.title(f'Confusion Matrix - SVM (C={best_C:.4f})', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Crime Type', fontsize=12)\n",
    "plt.ylabel('Actual Crime Type', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze where the model makes mistakes\n",
    "print(\"\\nModel Performance Analysis:\")\n",
    "print(\"=\"*60)\n",
    "for i, crime in enumerate(selected_crimes):\n",
    "    tp = cm_svm[i, i]\n",
    "    total_actual = cm_svm[i, :].sum()\n",
    "    recall = tp / total_actual if total_actual > 0 else 0\n",
    "    print(f\"\\n{crime}:\")\n",
    "    print(f\"  Correctly predicted: {tp}/{total_actual} ({recall:.1%})\")\n",
    "    \n",
    "    # Find most common misclassification\n",
    "    misclass_idx = [j for j in range(len(selected_crimes)) if j != i and cm_svm[i, j] > 0]\n",
    "    if misclass_idx:\n",
    "        max_misclass_idx = max(misclass_idx, key=lambda j: cm_svm[i, j])\n",
    "        print(f\"  Most often confused with: {selected_crimes[max_misclass_idx]} ({cm_svm[i, max_misclass_idx]} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 12. Confusion matrix for optimized linear SVM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adapted from our k-NN analysis - written by Tirth\n",
    "\n",
    "# Compare k-NNs vs SVMs\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': [f'Baseline k-NN(k=5)', \n",
    "            f'Optimized k-NN (k=85)',\n",
    "            f'Baseline SVM (C=1)', \n",
    "            f'Optimized SVM (C={best_C:.4f})'],\n",
    "    'Accuracy': [baseline_acc, final_acc, svm_base_acc, svm_score]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Simple bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(comparison['Model'], comparison['Accuracy'], \n",
    "               color=['lightblue', 'lightgreen', 'plum', 'pink'], edgecolor='black', linewidth=1.5)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 13. Comparison of model accuracies for k-NN and SVM baseline and optimized versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Analysis Summary\n",
    "\n",
    "**Model Performance:**\n",
    "- Baseline linear SVM: 64.2% accuracy\n",
    "- Optimized linear SVM: 64.3% accuracy\n",
    "- only 0.1% improvement through hyperparameter tuning\n",
    "\n",
    "**Key Findings:**\n",
    "- Linear SVM works very similarly to the optimized k-NN model for crime classification. \n",
    "- Linear SVM performs best on \"Break and Enter Residential/Other\" (f1-score = 1)\n",
    "- Linear SVM performs worst on \"Mischief\" (f1-score = 0.05)\n",
    "- hyperparameter optimization had almost no effect on linear SVM - the default value of C=1 worked just as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['HOUR', 'DAY_OF_WEEK', 'MONTH', 'DAY', 'IS_WEEKEND', \n",
    "                  'IS_RUSH_HOUR', 'IS_LATE_NIGHT', 'HOUR_SIN', 'HOUR_COS', 'MONTH_SIN', 'MONTH_COS']\n",
    "\n",
    "categorical_cols = [col for col in X_train.columns if col not in numerical_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', 'passthrough', categorical_cols) \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression pipeline\n",
    "logreg_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='saga',\n",
    "        max_iter=100\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1, 10],\n",
    "    'classifier__penalty': ['l1', 'l2']  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_logreg = GridSearchCV(logreg_pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_logreg.fit(X_train_sample, y_train_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_log_C = grid_logreg.best_params_['classifier__C']\n",
    "best_log_pen = grid_logreg.best_params_['classifier__penalty']\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_logreg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training final Log Reg model with best hyperparameters\n",
    "best_logreg = logreg_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='saga',\n",
    "        max_iter=100,\n",
    "        C=best_log_C,\n",
    "        penalty=best_log_pen\n",
    "    ))\n",
    "])\n",
    "\n",
    "best_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating the best model\n",
    "y_pred = best_logreg.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred, labels=best_logreg.classes_)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=best_logreg.classes_, yticklabels=best_logreg.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 14. Confusion matrix for the optimized logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adapted from our k-NN analysis - written by Tirth\n",
    "\n",
    "logreg_acc = accuracy_score(y_test, y_pred) \n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': [f'Baseline k-NN(k=5)', \n",
    "            f'Optimized k-NN (k=85)', \n",
    "            f'Optimized SVM (C={best_C:.4f})',\n",
    "            f'Tuned Logistic Regression'],\n",
    "    'Accuracy': [baseline_acc, 0.642, svm_score, logreg_acc]\n",
    "})\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Simple bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(comparison['Model'], comparison['Accuracy'], \n",
    "               color=['lightblue', 'lightgreen', 'pink', 'red'], edgecolor='black', linewidth=1.5)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 15. Comparison of all tested models: k-NN, SVM, Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The tuned multinomial Logistic Regression model achieved an overall accuracy of 62.8% on the Vancouver crime dataset. Performance varies by crime type: it predicts Offence Against a Person perfectly and Theft from Vehicle fairly well, but struggles with less frequent or ambiguous categories such as Break and Enter Residential/Other and Mischief, which show low recall and F1-scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### What We Found\n",
    "\n",
    "All three of our models ended up with pretty similar accuracy:\n",
    "- Baseline K-NN: ~58%\n",
    "- Optimized K-NN: 64.2%\n",
    "- Optimized SVM: 64.3%\n",
    "- Logistic Regression: 62.8%\n",
    "\n",
    "The models did well on common crimes like \"Theft from Vehicle\" but struggled with rarer or more ambiguous categories. We also noticed that crime in Vancouver decreased from 2003 to 2011, then started increasing again after 2012.\n",
    "\n",
    "### Was This Expected?\n",
    "\n",
    "Somewhat! Crime classification is tough because different crime types often happen in similar places at similar times. For example, various types of theft might all peak at night in the same neighborhoods. We're also only using time and location so we don't have info about weather, economic conditions, or other factors that might matter.\n",
    "\n",
    "The fact that all three algorithms performed similarly suggests the limiting factor isn't the algorithm choice, but rather what we can predict using only \"when\" and \"where\" information.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Even with 64% accuracy, these models could still be useful for:\n",
    "- Helping police decide where to patrol\n",
    "- Understanding which crime types are more predictable\n",
    "- Showing that we need more features beyond just time and location\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Possible next steps:\n",
    "- Add more features like weather, day of week, proximity to bars/schools\n",
    "- Try more advanced models like Random Forests or neural networks\n",
    "- Instead of predicting exact crime type, predict severity level (minor/moderate/serious)\n",
    "- Use more recent data (this dataset ends in 2017)\n",
    "- Apply techniques to handle the class imbalance problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Vancouver Police Department. (2017). *Crime Data* [Dataset]. Kaggle. https://www.kaggle.com/datasets/wosaku/crime-in-vancouver\n",
    "\n",
    "2. Scikit-learn developers. (2024). *Scikit-learn: Machine Learning in Python*. https://scikit-learn.org/\n",
    "\n",
    "3. Pandas development team. (2024). *pandas documentation*. https://pandas.pydata.org/docs/\n",
    "\n",
    "4. Altair developers. (2024). *Altair: Declarative Visualization in Python*. https://altair-viz.github.io/\n",
    "\n",
    "5. UBC Master of Data Science. (2025-26). *DSCI 571: Supervised Learning I*. University of British Columbia. https://github.com/UBC-MDS\n",
    "\n",
    "6. AI Coding Assistant: Claude AI agent in VS Code was used for debugging assistance and code optimization. All data analysis, model selection, and interpretation of results were completed independently by the authors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "522",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
